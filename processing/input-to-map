#!/usr/bin/python

import psycopg2
import psycopg2.extras
import os
import json
import re
from Stemmer import Stemmer

stemmer = Stemmer('english')
stop_words = [w.strip() for w in file("stop-words.txt").readlines()]

def stem(word):
    word = word.lower()
    for letter in "!?.,":
        word = word.replace(letter, "")
    return stemmer.stemWord(word)


def create_processed_tables(conn):
    print "*** Creating blank tables for output"
    cursor = conn.cursor()

    cursor.execute("DROP TABLE IF EXISTS person CASCADE")
    cursor.execute("CREATE TABLE person(id serial primary key, name text not null)")

    cursor.execute("DROP TABLE IF EXISTS word CASCADE")
    cursor.execute("CREATE TABLE word(id serial primary key, name text not null)")
    cursor.execute("CREATE INDEX stemmed_word on word(ts_lexize('english_stem', name))")

    cursor.execute("DROP TABLE IF EXISTS person_word CASCADE")
    cursor.execute("CREATE TABLE person_word(person_id integer references person(id), related_word_id integer references word(id), uses integer not null default 0)")

    cursor.execute("DROP TABLE IF EXISTS word_word CASCADE")
    cursor.execute("CREATE TABLE word_word(word_id integer references word(id), related_word_id integer references word(id), uses integer not null default 0)")

    cursor.execute("DROP TABLE IF EXISTS word_word_distance CASCADE")
    cursor.execute("CREATE TABLE word_word_distance(word_id integer references word(id), related_word_id integer references word(id), dist integer not null default 0, uses integer not null default 0)")

word_id_cache = {}

def get_word_id(conn, name):
    if name in word_id_cache:
        return word_id_cache[name]
    stemmed_name = stem(name)
    cursor = conn.cursor()
    cursor.execute("SELECT id FROM word WHERE ts_lexize('english_stem', name) = ts_lexize('english_stem', %s)", (stemmed_name, ))
    row = cursor.fetchone()
    if row:
        word_id = row[0]
    else:
        cursor.execute("INSERT INTO word(name) VALUES (%s) RETURNING id" , (name, ))
        word_id = cursor.fetchone()[0]
    word_id_cache[name] = word_id
    return word_id


person_words = {}

def add_person_word(conn, person_id, word_id):
    if person_id not in person_words:
        person_words[person_id] = {}
    if word_id not in person_words[person_id]:
        person_words[person_id][word_id] = 0
    person_words[person_id][word_id] = person_words[person_id][word_id] + 1
    #cursor = conn.cursor()
    #cursor.execute("SELECT uses FROM person_word WHERE person_id=%s AND related_word_id=%s", (person_id, word_id))
    #row = cursor.fetchone()
    #if row:
    #    cursor.execute("UPDATE person_word SET uses = uses + 1 WHERE person_id=%s AND related_word_id=%s", (person_id, word_id))
    #else:
    #    cursor.execute("INSERT INTO person_word (person_id, related_word_id, uses) VALUES (%s, %s, 1)", (person_id, word_id))


def commit_words(conn):
    cursor = conn.cursor()
    for person_id in person_words:
        print "Committing words for person %d" % person_id
        for word_id in person_words[person_id]:
            cursor.execute("INSERT INTO person_word (person_id, related_word_id, uses) VALUES (%s, %s, %s)", (person_id, word_id, person_words[person_id][word_id]))

word_distances = {}


def process(conn):
    print "*** Processing"
    cursor = conn.cursor()
    cursor.execute("DELETE FROM person")
    cursor.execute("INSERT INTO person(id, name) (SELECT DISTINCT speakerid, max(speakername) FROM input GROUP BY speakerid)")

    cursor.execute("SELECT timestamp, speakerid, speakername, text FROM input")
    swpa = {}
    for counter, (time, person_id, name, body) in enumerate(cursor):
        print "processed %d events" % counter
        for word in body.split():
            word = word.lower()
            if word not in stop_words:
                word_id = get_word_id(conn, word)
                add_person_word(conn, person_id, word_id)
        # TODO: relate words to other words
        #print "%s %20s %s" % (time, name, body[:40])
        sentences = body.split('.')
        for sentence in sentences:
            sentence_word_positions={}
            words = re.split('[^\w]', sentence)
            count = 0
            for i, word in enumerate(words):
                word = word.lower()
                if word in stop_words or i == 0 or len(word)==0:
                    continue
                word_id = get_word_id(conn, word)
                for ia in range(i-1):
                    word2 = words[ia].lower()
                    if word2 in stop_words or len (word2)==0:
                        continue
                    word_id2 = get_word_id(conn, word2)
                    word_id, word_id2 = sorted([word_id, word_id2])
                    key = (word_id, word_id2, i-ia)
                    swpa[key] = swpa.get(key, 0) + 1
    commit_words(conn)
    dist_len = len(swpa)
    i = 0
    for k,v in swpa.iteritems():
        print 'Commtted word_distance %s / %s' % (i, dist_len)
        i += 1
        w1, w2, d = k
        cursor.execute("INSERT INTO word_word_distance (word_id, related_word_id, dist, uses) VALUES (%s, %s, %s, %s)", (w1, w2, d, v))


def print_results(conn):
    print "*** Results"
    cursor = conn.cursor()
    cursor.execute("SELECT id, name FROM person")
    for person_id, person_name in cursor.fetchall():
        print "-", person_name
        cursor.execute("""
            SELECT related_word_id, name, uses
            FROM person_word JOIN word ON person_word.related_word_id = word.id
            WHERE person_id = %s
            ORDER BY uses DESC
            LIMIT 10
        """, (person_id, ))
        for word_id, word_name, uses in cursor.fetchall():
            print uses, word_name
            pass


def main():
    conn_string = "host='localhost' dbname='%(user)s' user='%(user)s' password='%(user)s'" % {'user': os.environ['USER']}
    conn = psycopg2.connect(conn_string)

    create_processed_tables(conn)
    process(conn)
    print_results(conn)

    conn.commit()

if __name__ == "__main__":
    main()

